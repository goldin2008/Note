## Presentation and Note
### Introduction
Digitized documents have become an omnipresent medium of information. A plethora of scholarly documents on the web is excessively being increased. Most of the scientific literature is stored in Portable Document Format (PDF). PDF documents hold a complex structure due to which their comprehension and extraction of useful information from them is a challenging task. In this regard, research community has been proposing different rule based and machine learning based techniques in the past several years. We believe that accurate and efficient information extraction form the PDF files is an important issue as major portion of scholarly literature is stored in PDF.

To help a soil science team from the United States Department of Agriculture (USDA) build a queryable journal paper system, we used web crawler with Python to download journal papers on soil science from the digital library to provide users with papers they are interested in. To extract useful information including authors, journal, publish date, abstract, DOI, journal type, experiment location and key words in papers and highlight the paper characteristics in data system, we applied named entity recognition to extract authors and location of experiments, table analysis to extract tables in the paper. The named entity recognition technique is used to extract authors and experiment location. And the table analysis is used to store the tables from the journal paper in a computer queryable form. Text analysis is applied to figure out the parts of interest, and stored them in the database to save time. We used traditional machine learning techniques including logistic regression, support vector machine, decision tree, naive bayes, k-nearest neighbors, random forest, ensemble modeling, and neural networks in text analysis and compare the advantages of these approaches in the end.

In an age of rapidly increasing numbers of published scientific articles, it is surprising that most systematic literature reviews and extraction of information from tables are still conducted by manually processing articles individually \cite{fut1}. Systematic literature reviews aim to find and collect relevant information concerning a specific research question and are an essential step in virtually every area of research, e.g., for the preparation of review articles, project proposals, and experimental designs. While machine learning tools are available for literature searches and screens \cite{context}, they require a large number of manually evaluated articles for the training of the tool. They are often restricted to filtering articles by study design or choosing topics from a limited set of terms, and are generally limited to the evaluation of article titles and abstracts.

To extract information from journals automatically and easily, a soil science team from the United States Department of Agriculture (USDA) want to build a queryable journal paper data system, where users can easily identify journal papers of interest to them. To satisfy their requirements, the system needs information including authors, journal, publish date, abstract, DOI, journal type, experiment location and key words in papers to highlight the paper characteristics. This information will help users to figure out if the paper is of interest to them and locate it quickly if needed. The important initial factor is data, which is journal papers in the system. We used web crawler with Python to download journal papers on soil science from the digital library to provide users with papers which is of interest to them. To extract useful information from journal papers and store them in data system as indexing and abstract, we applied name entity recognition to extract authors and location of experiments, table analysis to extract tables in the paper and store the them in a computer queryable form.

To make system recommend journal papers to users automatically, we built machine learning and deep learning models to identify users' interests. Text analysis is applied on the text data to figure out what parts of paper the user are interested in, and stored them in the database to save users' search time. During this part, I fed the text data including sections, paragraphs to many types of machine learning algorithms and used the trained models to classify unseen data in order to help user distinguish if the new pieces of text in journal paper is useful. I used traditional machine learning techniques including logistic regression, support vector machine, decision tree, naive bayes, k-nearest neighbors, random forest, ensemble modeling, and neural networks in text analysis and compare the advantages of these approaches in the end.

My contributions consist of five parts: Web harvesting, Text Classification, Table Analysis, Named Entity Recognition and Database System Building. Finally all of them can populate a relational database with information automatically extracted from journal papers collected from internet resources, and send users proper recommendations. The reason we used a web crawler to download papers is we need collect papers to build database and the papers are also the basis of the following tasks. Text classification can help identify the section or paragraph in a paper that may be of interest to users based on their own search interest. Named Entity Recognition can extract author and experiment location from paper to store them in data system, and database system will make the future query more efficiently.

### Web Scraping


### Data Statistical Description


### Text Classification
Text classification is a way to categorize documents or pieces of text. By examining the word usage in a piece of text, classifiers can decide what class label to assign to it. A binary classifier decides between two labels, such as positive review or negative review, desirable or not desirable information. The text can either be one label or another, but not both. The purpose of text classification in this project is to classify the unknown journal paper or pieces of text in it as desirable information or not by training on already highlighted desirable documents, in order to save the users’ new paper seeking time and save the desirable information in queryable database.

We need to first convert PDF to text format, since the text can not be read directly from PDF format. There are many conversion tools and a lot of variance in output quality. Converting PDF to text is one of the most common features for standard PDF converting tool. However, there could be great difference in output quality. In our daily documents processing, PDF that with multi-column text is somehow inevitable. Unfortunately, many PDF Text converters handle single column text well but fail miserably when presented with a typical multiple-column layout by interlacing the multiple columns. For these journal papers, we need to clean the text, since after conversion from PDF format the text would get scrambled, with pieces of left column being mixed with the right one. Some papers have three columns, making the problem more serious. Another common problem is that the position of splitting is not fixed. Part of content in the first paragraph may be split to the second, or even third paragraph. These would make cleaning text tough.

In this section, we delve into text analysis and use machine learning algorithms to classify documents or pieces of text (sentence, paragraph, section) based on the attitude or emotions of the end user, like interested in them or not. The details of machine learning algorithms and performance evaluation metrics we used here are in section 3.
For the section classification problem, it consists of 1690 files that are labeled as 1177 positive and 513 negative, where positive means that the user is interested in that text and negative means that the user is not interested in that text. `And for the paragraph classification task, it consists of 7543 files (6045 positives and 1498 negatives).`
I spent a lot of time on the labeling process since the documents are labeled manually. The positves and negatives are placed on different folders. After we got these files, we preprocess them into a useable format for machine learning algorithms, and extract meaningful information from them to feed to models. Then we use these models to predict whether the user is interested in the text or not.

1. To handle text data easier, we read the text data into a pandas DataFrame object and it gives more structured data and better visualization.
2. Clean Data: We first clean numbers, punctuation marks, and other non letter characters in the text data, since they do not contain much useful semantic information in our project.
3. Tokenization: Tokenization is the process of breaking down a text corpus into individual elements that serve as input for various natural language processing algorithms. Usually, tokenization is accompanied by other optional processing steps, such as the removal of stop words and punctuation characters, stemming or lemmatizing, and the construction of n-grams.
4. Stop Words: We remove the stop words, since they are pretty common in all kinds of texts and do not contain much useful information for document classification. NLTK library has a set of 127 English stop words. And we could use it to remove stop words in the text.
5. Lowercase: Then we convert the text into lowercase characters, since the semantic information does not depend on whether the word is at the start of the sentence or not. Another reason is our model does not distinguish the letter case difference, since unigram bag-of-words model does not concern the order of the words.
6. Stemming and Lemmatization: Stemming describes the process of transforming a word into its root form. The original stemming algorithm was developed my Martin F. Porter in 1979 and is hence known as Porter stemmer \cite{stem1}. Stemming can create non-real words, such as “thu” in the example above. In contrast to stemming, lemmatization aims to obtain the canonical (grammatically correct) forms of the words, the so-called lemmas. Lemmatization is computationally more difficult and expensive than stemming.
7. N-Grams: In the n-gram model \cite{nb}, a token can be defined as a sequence of n items. The simplest case is the so-called unigram (1-gram) where each token consists of exactly one word, letter, or symbol. Choosing the optimal number n depends on the language as well as the particular application. For example, Andelka Zecevic found in his study that n-grams with $3\leq n \leq 7$ were the best choice to determine authorship of Serbian text documents \cite{stem2}. In a different study, the n-grams of size $4\leq n \leq 8$ yielded the highest accuracy in authorship determination of English text books \cite{stem3} and Kanaris and others report that n-grams of size 3 and 4 yield good performances in anti-spam filtering of e-mail messages \cite{stem4}. In our work, we chose range $1$ to $3$ as n-gram grid search search to balance train time and performance due to compute resource limit.
8. Fine Tuning Hyperparameters: In machine learning, we have two types of parameters: One are the parameters that the machine learning algorithm learned from the training data like the weights in the logistic regression, neural network, which we would get in the training step. The other are tuning parameters, which are called hyperparameters, like the regularization parameter in the logistic regression, the maximum depth of a decision tree and number of estimators in the random forest. Now we need to tune the hyperparameters in our machine learning models. We use a grid search to find the optimal set of parameters by finding the optimal combination of hyperparameters values for model using stratified 10-fold cross-validation. The reason why we use stratified 10-fold cross-validation instead of the standard 10-fold cross-validation is that our dataset has unequal class proportions. In the stratified 10-fold cross-validation, the class proportions are preserved in each fold to ensure that each fold is representative of the class proportions in the training dataset, and this would yield better bias and variance estimates on this type of dataset.
9. The approach of grid search is a brute force exhaustive search paradigm where we specify a list of values for different hyperparameters, and the computer evaluates the model performance for each combination of those to obtain the optimal combination of values. Here, we use 10-fold cross-validation for tuning hyperparameters, since it would help to find the optimal hyperparameter values that yields a satisfying generalization performance. 

The purpose of Text Classification is that we feed model with an unseen journal paper, and this classifier model could identify if the new paper or part of it is what users are interested in and store the content in the database if needed. We do two experiments there, one for section text classification, and the other paragraph text classification. Section text classification indicates the classifier models would consider each section in the journal paper as a block and classify it as desirable or not. There are 1690 files in section text classification. Paragraph text classification denotes that the classifier models would consider each paragraph in the journal paper as a block and classify it as desirable or not. 
`There are 7543 files in paragraph text classification.`

For section text classification, we have total 1690 files, and 513 of them are content of interest, and the other 1177 are not needed. 30\% of them are what users are interested in. By our classifier model, we could classify about 92\% of them correctly. 
`For paragraph text classification, we have total 7543 files, and 1498 of them are desired, and the other 6045 are not needed. 20\% of them are what users are interested in. By our classifier model, we could classify almost 90\% of them correctly.`
According to our experiment results like F-measure, the less content, the more difficulty to classify correctly. This makes sense since the less information we have, the more difficulty for us to make correct decision.

`Deep Learning`
We can see LSTM achieved the best performance and it shows us that users will get 1 paper which they are interested in given every 2.89 recommendations.

Figure \ref{fig:dl_plot} shows the accuracy and loss during training and testing and we can see the gap between train and test makes sense and does not trigger overfitting. To build robust model, we need to catch all true positives and reduce false positives. Figure \ref{fig:dl_cutoff} shows how we search the optimum cutoff to achieve this goal. The top two plots are for Percentage, while the bottom two are for Counts. They give us a clear tracking during the search. We search twice, the first search window is $0$ to $1$ which are probabilities of class 1 (interest). Then we narrow the search window and get a preciser cutoff, since a tiny cutoff change can change the model performance a lot as shown in the Figure \ref{fig:dl_cutoff}.

CM shows we catch all positives and 195 (83\%) negatives. This means every 2.89 suggested papers, users can get 1 which they are interested. 2.89 is calculated on (103+195)/103. Because the data is imbalanced, accuracy is not a good metric for model evaluation. Our goal is to make sure all true positives can be identified since we hope the model does not miss any piece of text which users are interested in, while reduce the false positives since they are undesirable informaiton to users. To achieve this purpose, we built a custom metric which can catch all true positives and reduce false positives as many as it could.

### Named Entity Recognition


### Table Analysis


### Database System


### Conclusion
The contributions of this thesis are:
1. Web harvesting: We downloaded 38,444 papers with size of 29.53 GB from Digital Library at University of Nebraska Lincoln.
2. Text Classification: We built a machine learning-based system to identify the sections or paragraphs in the papers that may be of interest to users based on their own search interest. The model we built can catch all positives and 83\% negatives. This means there is 1 paper of interest to users for every 2.89 suggested papers.
3. Table Analysis: After manually creating different kinds of tables in the journal papers, we used Seth et al.'s approach and found the program could process about 90\% of all tables, which are well-formed tables. For the other 10\% not well-formed tables, the program can not extract correct information.
4. Named Entity Recognition: We used Stanford's Named Entity Recognition to extract author and experiment location from paper and store them in data system which will make the future query more efficiently. The accuracy can reach about 83\%.
5. Database System Building: We stored the journal paper related information including Title, Publication Date, Abstract, Journal, DOI and Type, authors, city and state extracted from papers by Stanford NER, the count of the occurrences of terms of interest to soil scientists, and infromation contained in the well-formed table converted by the algorithm of Seth et al. \cite{t1} in the Microsoft Access Database.
