## Presentation and Note
### Introduction
Digitized documents have become an omnipresent medium of information. A plethora of scholarly documents on the web is excessively being increased. Most of the scientific literature is stored in Portable Document Format (PDF). PDF documents hold a complex structure due to which their comprehension and extraction of useful information from them is a challenging task. In this regard, research community has been proposing different rule based and machine learning based techniques in the past several years. We believe that accurate and efficient information extraction form the PDF files is an important issue as major portion of scholarly literature is stored in PDF.

To help a soil science team from the United States Department of Agriculture (USDA) build a queryable journal paper system, we used web crawler with Python to download journal papers on soil science from the digital library to provide users with papers they are interested in. To extract useful information including authors, journal, publish date, abstract, DOI, journal type, experiment location and key words in papers and highlight the paper characteristics in data system, we applied named entity recognition to extract authors and location of experiments, table analysis to extract tables in the paper. The named entity recognition technique is used to extract authors and experiment location. And the table analysis is used to store the tables from the journal paper in a computer queryable form. Text analysis is applied to figure out the parts of interest, and stored them in the database to save time. We used traditional machine learning techniques including logistic regression, support vector machine, decision tree, naive bayes, k-nearest neighbors, random forest, ensemble modeling, and neural networks in text analysis and compare the advantages of these approaches in the end.

In an age of rapidly increasing numbers of published scientific articles, it is surprising that most systematic literature reviews and extraction of information from tables are still conducted by manually processing articles individually \cite{fut1}. Systematic literature reviews aim to find and collect relevant information concerning a specific research question and are an essential step in virtually every area of research, e.g., for the preparation of review articles, project proposals, and experimental designs. While machine learning tools are available for literature searches and screens \cite{context}, they require a large number of manually evaluated articles for the training of the tool. They are often restricted to filtering articles by study design or choosing topics from a limited set of terms, and are generally limited to the evaluation of article titles and abstracts.

To extract information from journals automatically and easily, a soil science team from the United States Department of Agriculture (USDA) want to build a queryable journal paper data system, where users can easily identify journal papers of interest to them. To satisfy their requirements, the system needs information including authors, journal, publish date, abstract, DOI, journal type, experiment location and key words in papers to highlight the paper characteristics. This information will help users to figure out if the paper is of interest to them and locate it quickly if needed. The important initial factor is data, which is journal papers in the system. We used web crawler with Python to download journal papers on soil science from the digital library to provide users with papers which is of interest to them. To extract useful information from journal papers and store them in data system as indexing and abstract, we applied name entity recognition to extract authors and location of experiments, table analysis to extract tables in the paper and store the them in a computer queryable form.

To make system recommend journal papers to users automatically, we built machine learning and deep learning models to identify users' interests. Text analysis is applied on the text data to figure out what parts of paper the user are interested in, and stored them in the database to save users' search time. During this part, I fed the text data including sections, paragraphs to many types of machine learning algorithms and used the trained models to classify unseen data in order to help user distinguish if the new pieces of text in journal paper is useful. I used traditional machine learning techniques including logistic regression, support vector machine, decision tree, naive bayes, k-nearest neighbors, random forest, ensemble modeling, and neural networks in text analysis and compare the advantages of these approaches in the end.

My contributions consist of five parts: Web harvesting, Text Classification, Table Analysis, Named Entity Recognition and Database System Building. Finally all of them can populate a relational database with information automatically extracted from journal papers collected from internet resources, and send users proper recommendations. The reason we used a web crawler to download papers is we need collect papers to build database and the papers are also the basis of the following tasks. Text classification can help identify the section or paragraph in a paper that may be of interest to users based on their own search interest. Named Entity Recognition can extract author and experiment location from paper to store them in data system, and database system will make the future query more efficiently.

### Text Classification
Text classification is a way to categorize documents or pieces of text. By examining the word usage in a piece of text, classifiers can decide what class label to assign to it. A binary classifier decides between two labels, such as positive review or negative review, desirable or not desirable information. The text can either be one label or another, but not both. The purpose of text classification in this project is to classify the unknown journal paper or pieces of text in it as desirable information or not by training on already highlighted desirable documents, in order to save the usersâ€™ new paper seeking time and save the desirable information in queryable database.

### Conclusion
The contributions of this thesis are:
1. Web harvesting: We downloaded 38,444 papers with size of 29.53 GB from Digital Library at University of Nebraska Lincoln.
2. Text Classification: We built a machine learning-based system to identify the sections or paragraphs in the papers that may be of interest to users based on their own search interest. The model we built can catch all positives and 83\% negatives. This means there is 1 paper of interest to users for every 2.89 suggested papers.
3. Table Analysis: After manually creating different kinds of tables in the journal papers, we used Seth et al.'s approach and found the program could process about 90\% of all tables, which are well-formed tables. For the other 10\% not well-formed tables, the program can not extract correct information.
4. Named Entity Recognition: We used Stanford's Named Entity Recognition to extract author and experiment location from paper and store them in data system which will make the future query more efficiently. The accuracy can reach about 83\%.
5. Database System Building: We stored the journal paper related information including Title, Publication Date, Abstract, Journal, DOI and Type, authors, city and state extracted from papers by Stanford NER, the count of the occurrences of terms of interest to soil scientists, and infromation contained in the well-formed table converted by the algorithm of Seth et al. \cite{t1} in the Microsoft Access Database.
