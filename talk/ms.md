## Presentation and Note
### Motivation
To extract information from journals automatically and easily, a soil science team from the United States Department of Agriculture (USDA) want to build a queryable journal paper data system, where users can easily identify journal papers of interest to them. 

### Introduction
To make system recommend journal papers to users automatically, we built machine learning and deep learning models to identify users' interests. Text analysis is applied on the text data to figure out what parts of paper the user are interested in, and stored them in the database to save users' search time.

We followed Prof. Seth's well formed table requirement and create tables in CSV format manually and used his program to store the table in a queryable machine readable format. The users can query and search key words or highlight some section or paragraph in the paper, then the system will provide the relevant information or recommend a paper which is of interest to users. This system will make future queries more efficient.

### Background
The diagonal of a ROC graph can be interpreted as random guessing and classification models that fall below the diagonal are considered as worse than random guessing. A perfect classifier would fall into the top left corner of the graph with a True Positive Rate of $1$ and a False Positive Rate of $0$. Based on the ROC curve, the so-called Area Under the Curve (AUC) can be used to calculate the performance of a classification model. The bigger AUC value, the better classification model.

`ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets. In both cases the area under the curve (AUC) can be used as a summary of the model performance.`

### Web Scraping


### Data Statistical Description
We have in total 207 papers and divide each paper into sections to apply text classification on them. 
The total number of files is 1690. The total number of journals is 207. And the average files per journal is 8.2. The labels in the table \ref{tab:intro1} and \ref{tab:intro2} indicate if users are interested in the content.

To test if I can use text classification on less information, I divide each paper into paragraphs which contain less information than sections.  
The total number of files is 7543. The total number of journals is 207. And the average files per journal is 36.4. The labels in the table \ref{tab:intro2} indicate if users are interested in the content, where 0 means not interest and 1 means interest. 

Table \ref{tab:intro1} and Table \ref{tab:intro2} show the text description in our raw data, and they only show the top 5 files in section and paragraph text, due to limit space. Journal column indicates the journal index whose range is from 001 to 207. Label denotes if the content is relevant with researchers' interest, where 0 means no interest, while 1 means interest. File Name count column shows how many files in each label, for example, 5 means there are 5 files in Journal 001 which the researchers are interested in. Let still use the first row in Table \ref{tab:intro1} as an example. Number of characters in the clean text is 304 in minimum, 17013 in maximum, sum is 33326, and 6665 on average. Number of words in the clean text is 42 in minimum, 2648 in maximum, sum is 5047, and 1009 on average. Figure \ref{fig:d1} and Figure \ref{fig:d2} indicates the distribution of labels, distribution of number of words, and distribution of number of characters in section and paragraph data.

These two tables and distribution Figure \ref{fig:d1} and Figure \ref{fig:d2} provide a outline to show us what the data looks like.

### Text Classification
Text classification is a way to categorize documents or pieces of text. By examining the word usage in a piece of text, classifiers can decide what class label to assign to it. A binary classifier decides between two labels, such as positive review or negative review, desirable or not desirable information. The text can either be one label or another, but not both. The purpose of text classification in this project is to classify the unknown journal paper or pieces of text in it as desirable information or not by training on already highlighted desirable documents, in order to save the users’ new paper seeking time and save the desirable information in queryable database.

We need to first convert PDF to text format, since the text can not be read directly from PDF format. There are many conversion tools and a lot of variance in output quality. Converting PDF to text is one of the most common features for standard PDF converting tool. However, there could be great difference in output quality. In our daily documents processing, PDF that with multi-column text is somehow inevitable. Unfortunately, many PDF Text converters handle single column text well but fail miserably when presented with a typical multiple-column layout by interlacing the multiple columns. For these journal papers, we need to clean the text, since after conversion from PDF format the text would get scrambled, with pieces of left column being mixed with the right one. Some papers have three columns, making the problem more serious. Another common problem is that the position of splitting is not fixed. Part of content in the first paragraph may be split to the second, or even third paragraph. These would make cleaning text tough.

In this section, we delve into text analysis and use machine learning algorithms to classify documents or pieces of text (sentence, paragraph, section) based on the attitude or emotions of the end user, like interested in them or not. The details of machine learning algorithms and performance evaluation metrics we used here are in section 3.
For the section classification problem, it consists of 1690 files that are labeled as 1177 positive and 513 negative, where positive means that the user is interested in that text and negative means that the user is not interested in that text. `And for the paragraph classification task, it consists of 7543 files (6045 positives and 1498 negatives).`
I spent a lot of time on the labeling process since the documents are labeled manually. The positves and negatives are placed on different folders. After we got these files, we preprocess them into a useable format for machine learning algorithms, and extract meaningful information from them to feed to models. Then we use these models to predict whether the user is interested in the text or not.

1. To handle text data easier, we read the text data into a pandas DataFrame object and it gives more structured data and better visualization.
2. Clean Data: We first clean numbers, punctuation marks, and other non letter characters in the text data, since they do not contain much useful semantic information in our project.
3. Tokenization: Tokenization is the process of breaking down a text corpus into individual elements that serve as input for various natural language processing algorithms. Usually, tokenization is accompanied by other optional processing steps, such as the removal of stop words and punctuation characters, stemming or lemmatizing, and the construction of n-grams.
4. Stop Words: We remove the stop words, since they are pretty common in all kinds of texts and do not contain much useful information for document classification. NLTK library has a set of 127 English stop words. And we could use it to remove stop words in the text.
5. Lowercase: Then we convert the text into lowercase characters, since the semantic information does not depend on whether the word is at the start of the sentence or not. Another reason is our model does not distinguish the letter case difference, since unigram bag-of-words model does not concern the order of the words.
6. Stemming and Lemmatization: Stemming describes the process of transforming a word into its root form. The original stemming algorithm was developed my Martin F. Porter in 1979 and is hence known as Porter stemmer \cite{stem1}. Stemming can create non-real words, such as “thu” in the example above. In contrast to stemming, lemmatization aims to obtain the canonical (grammatically correct) forms of the words, the so-called lemmas. Lemmatization is computationally more difficult and expensive than stemming.
7. N-Grams: In the n-gram model \cite{nb}, a token can be defined as a sequence of n items. The simplest case is the so-called unigram (1-gram) where each token consists of exactly one word, letter, or symbol. Choosing the optimal number n depends on the language as well as the particular application. For example, Andelka Zecevic found in his study that n-grams with $3\leq n \leq 7$ were the best choice to determine authorship of Serbian text documents \cite{stem2}. In a different study, the n-grams of size $4\leq n \leq 8$ yielded the highest accuracy in authorship determination of English text books \cite{stem3} and Kanaris and others report that n-grams of size 3 and 4 yield good performances in anti-spam filtering of e-mail messages \cite{stem4}. In our work, we chose range $1$ to $3$ as n-gram grid search search to balance train time and performance due to compute resource limit.
8. Fine Tuning Hyperparameters: In machine learning, we have two types of parameters: One are the parameters that the machine learning algorithm learned from the training data like the weights in the logistic regression, neural network, which we would get in the training step. The other are tuning parameters, which are called hyperparameters, like the regularization parameter in the logistic regression, the maximum depth of a decision tree and number of estimators in the random forest. Now we need to tune the hyperparameters in our machine learning models. We use a grid search to find the optimal set of parameters by finding the optimal combination of hyperparameters values for model using stratified 10-fold cross-validation. The reason why we use stratified 10-fold cross-validation instead of the standard 10-fold cross-validation is that our dataset has unequal class proportions. In the stratified 10-fold cross-validation, the class proportions are preserved in each fold to ensure that each fold is representative of the class proportions in the training dataset, and this would yield better bias and variance estimates on this type of dataset.
9. The approach of grid search is a brute force exhaustive search paradigm where we specify a list of values for different hyperparameters, and the computer evaluates the model performance for each combination of those to obtain the optimal combination of values. Here, we use 10-fold cross-validation for tuning hyperparameters, since it would help to find the optimal hyperparameter values that yields a satisfying generalization performance. 

The purpose of Text Classification is that we feed model with an unseen journal paper, and this classifier model could identify if the new paper or part of it is what users are interested in and store the content in the database if needed. We do two experiments there, one for section text classification, and the other paragraph text classification. Section text classification indicates the classifier models would consider each section in the journal paper as a block and classify it as desirable or not. There are 1690 files in section text classification. Paragraph text classification denotes that the classifier models would consider each paragraph in the journal paper as a block and classify it as desirable or not. 
`There are 7543 files in paragraph text classification.`

For section text classification, we have total 1690 files, and 513 of them are content of interest, and the other 1177 are not needed. 30\% of them are what users are interested in. By our classifier model, we could classify about 92\% of them correctly. 
`For paragraph text classification, we have total 7543 files, and 1498 of them are desired, and the other 6045 are not needed. 20\% of them are what users are interested in. By our classifier model, we could classify almost 90\% of them correctly.`
According to our experiment results like F-measure, the less content, the more difficulty to classify correctly. This makes sense since the less information we have, the more difficulty for us to make correct decision.

`Deep Learning`
We can see LSTM achieved the best performance and it shows us that users will get 1 paper which they are interested in given every 2.89 recommendations.

Figure \ref{fig:dl_plot} shows the accuracy and loss during training and testing and we can see the gap between train and test makes sense and does not trigger overfitting. To build robust model, we need to catch all true positives and reduce false positives. Figure \ref{fig:dl_cutoff} shows how we search the optimum cutoff to achieve this goal. The top two plots are for Percentage, while the bottom two are for Counts. They give us a clear tracking during the search. We search twice, the first search window is $0$ to $1$ which are probabilities of class 1 (interest). Then we narrow the search window and get a preciser cutoff, since a tiny cutoff change can change the model performance a lot as shown in the Figure \ref{fig:dl_cutoff}.

CM shows we catch all positives and 195 (83\%) negatives. This means every 2.89 suggested papers, users can get 1 which they are interested. 2.89 is calculated on (103+195)/103. Because the data is imbalanced, accuracy is not a good metric for model evaluation. Our goal is to make sure all true positives can be identified since we hope the model does not miss any piece of text which users are interested in, while reduce the false positives since they are undesirable informaiton to users. To achieve this purpose, we built a custom metric which can catch all true positives and reduce false positives as many as it could.

### Named Entity Recognition
Named entities are definite noun phrases that refer to specific types of individuals, such as organizations, persons, locations, geo-political entities, date, percent etc. As shown in Figure \ref{fig:ner}, the purpose of NER is to identify all named entities. We used Stanford NER \cite{ner}, which is a Java implementation of a Named Entity Recognizer, to identify persons and locations contained in the journal. 

The reason why we want to extract named entities from journal papers is that we want to select papers by the locations contained in the paper. For example, to be able to query studies that were performed in Lancaster County, Nebraska, so one could view journal papers and results associated with that location as shown in Figure \ref{fig:ner12}. Named Entity Recognition can help us make it a reality. It would extract information about references to PERSONs, ORGANIZATIONs, and LOCATIONs from journal papers. The LOCATION will include all locations mentioned in the paper.  We also want to extract information about the authors and organizations, and this information is contained in PERSONs, ORGANIZATIONs. As shown in the Figure \ref{fig:ner12}, user can select Lincoln as Location and the system will list all papers whose experiments take place at Lincoln, Nebraska.

While PDFs provide an easily readable presentation of data, they are extremely difficult to work with in data analysis. We used a Pdfminer called “pdf2txt.py” that extracts text contents from a PDF file, and NLTK (the Natural Language Tool Kit) \cite{stem0} serves as one of Python’s leading platforms to analyze natural language data. In Named Entity Recognition, we ignore the references section since it contains too many persons and organizations which are not related to the desired information. We also used Stanford's Named Entity Recognizer \cite{ner}, often called Stanford NER, to extract PERSONs, LOCATIONs, and ORGANIZAIONs from journal papers and store them in JSON format. NLTK contains an interface to Stanford NER, so all codes are written in Python. 

On the left side of the Figure \ref{fig:ner22}, the column is from hand labeled location file. Data on the right side is the output of the Stanford's Named Entity Recognizer. Among location list on the left side of the Figure \ref{fig:ner22}, Stanford's Named Entity Recognizer can identify $10$ items including Multan, USA, Miami, Islamabad, Ohio, Pakistan, Columbus, Germany, Coffey Road and Hanau. The missed items are OH and south Brazil, where OH is abbreviation of Ohio. Stanford's Named Entity Recognizer can identify $10$ out of $12$, which is about $83\%$. Based on the precision, recall and F-score formulas, we can get that precision is $1$, recall is $0.83$ and F-score is about $0.91$. In Manning et al.'s paper \cite{ner}, they claim that F-scores are $0.87$ and $0.92$ for the CoNLL and CMU Seminar Announcements respectively, where the CoNLL is named entity recognition task and the
CMU Seminar Announcements is information extraction task for NER approach evaluation. The NER performance is in between these two scores for our project and it makes sense. It is better if we could use more samples to test the performance of Stanford's Named Entity Recognizer and verify the approach works well in our project. However, it will cost a lot of labeling time and effort and we do not have enough time or labor to do that.

### Table Analysis
In Seth et al.'s algorithm \cite{t1}, critical cells (CC1, CC2, CC3, CC4) delineate regions. In a WFT every critical cell must appear in the grid. As shown in Figure \ref{fig:wft}, CC1 and CC2 demarcate the StubHeader and CC3 and CC4 demarcate the Data region. Furthermore, in combination with one another, these critical cells also demarcate both the ColHeader and RowHeader regions. Letting row \textit{$r_i$} and column \textit{$c_i$} be the coordinates of critical cell CCi, a WFT satisfies the following constraints: $r_1 \leq r_2 < r_3 \leq r_4$ and $c_1 \leq c_2 < c_3 \leq c_4$. These constraints guarantee that the ColHeader and RowHeader regions properly align with the Data region and that the Data region is not degenerate. A single row or column of data is acceptable, provided both row and column headers exist.

They do not deal here with concatenated (composite) tables, nested tables (tables whose data cells may themselves be tables), tables containing graphic data, or “egregious” tables (those not laid out on a grid with headers above and to the left).

Their program could transform well-formed tables to a new canonical table format via: segmenting table regions by algorithmic data cell indexing, factoring header paths into categories by algorithmic header analysis, and generating queryable canonical relational tables.

In a well-formed table (WFT), every data cell is uniquely indexed by its row and column header paths, which are respectively left of and above the data region. A hierarchical (row or column) header may index one or more categories. A single-category header path consists of the root-to-leaf path of the corresponding category tree. A multi-category header path consists of concatenated category paths.

Tables contain data of interest to users. The purpose of table analysis is to convert the content of human-readable tables in the journal paper to query-table in database. We created 1006 tables for 207 journal papers in Comma-Separated Values (CSV) format manually. Then we ran the algorithm of Seth et al. to extract data from CSV and store them in a machine-readable format. 
`There are 1006 tables in the 207 papers. Each table would cost 20-30 minutes, we test 50\% of them, so the total time spent on preparing and checking these 500 tables is about 280 hours.`

The program can output two kinds of tables. One is a classification table. This table is in a five-column format, with a row entry (after the header row) for each cell of its source table. The first column is a unique cell identifier with the file name of the CSV table and the cell coordinates. The second and third rows give the numerical cell coordinates separately for ease of handling. The fourth column is the content of the cell in the original table, and the last column is its assigned class.\\

The other table is a category table which is a relational table where each row comprises the indexing header paths and the corresponding indexed data value. Therefore the number of rows in the category table equals the number of data cells in the original table (plus one for the relational table’s field names in a header row). The number of columns is one for the Cell$\_$ID, plus one for DATA, plus the sum of the heights of the category trees (which, usually, equals the sum of the column width of the row header and row height of the column header). In the category table, Cell$\_$ID is a key field and each cell label in the original header paths becomes a key field value in the composite key comprising all the category fields.

### Database System
We used Stanford NER to extract city and state from papers, wrote code to extract first five authors, Title, Publication Date, Abstract, Journal, DOI and Type from papers, and count if keywords defined by soil scientist appear in the paper. Then we used the algorithm of Seth et al. \cite{t1} to convert the well-formed table (manually created by myself) to tables. Finally, we inserted all information mentioned above into Microsoft Access Database.

keywords are defined by soil scientist and they are used to count the occurrences of terms that soil scientists are interested in, including Conservation, No Tillage, Ridge Tillage, Mulch Tillage, Strip Tillage, Reduced Till etc. in Keyword 1 list and Germanium, Gold, Hafnium, Hassium etc. in Keyword 2 list. 

### Conclusion
The contributions of this thesis are:
1. Web harvesting: We downloaded 38,444 papers with size of 29.53 GB from Digital Library at University of Nebraska Lincoln.
2. Text Classification: We built a machine learning-based system to identify the sections or paragraphs in the papers that may be of interest to users based on their own search interest. The model we built can catch all positives and 83\% negatives. This means there is 1 paper of interest to users for every 2.89 suggested papers.
3. Table Analysis: After manually creating different kinds of tables in the journal papers, we used Seth et al.'s approach and found the program could process about 90\% of all tables, which are well-formed tables. For the other 10\% not well-formed tables, the program can not extract correct information.
4. Named Entity Recognition: We used Stanford's Named Entity Recognition to extract author and experiment location from paper and store them in data system which will make the future query more efficiently. The accuracy can reach about 83\%.
5. Database System Building: We stored the journal paper related information including Title, Publication Date, Abstract, Journal, DOI and Type, authors, city and state extracted from papers by Stanford NER, the count of the occurrences of terms of interest to soil scientists, and infromation contained in the well-formed table converted by the algorithm of Seth et al. \cite{t1} in the Microsoft Access Database.


Committee: \\
Prof. Stephen D. Scott \\
Prof. Vinodchandran Variyam \\
Prof. Ashok Samal \\
